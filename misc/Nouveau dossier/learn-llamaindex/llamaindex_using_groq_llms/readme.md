Create RAG pipeline with LlamaIndex, Llama3 and Groq in 20 mins #groq #llama3 #llamaindex #llm

Super Lazy Coder
2.25K subscribers

Join

Subscribe

25


Share

Save

1,020 views  Jun 10, 2024
LlamaIndex is a framework for building context-augmented LLM applications. Context augmentation refers to any use case that applies LLMs on top of your private or domain-specific data. This includes Question-Answering Chatbots, Document Extraction and many more. 
In this video we will understand LlamaIndex and its comparison with Langchain. We will also create a RAG pipeline from scratch with Llama3 and Groq.

Thank you for watching please like share and subscribe!!

0:00 Introduction
0:15 Llama Index vs Langchain
2:09 What is RAG
3:21 Use cases of Llama Index and langchain
5:13 Llama index installation and documentation
6:09 Creating RAG pipeline with Llama Index and Groq
6:12 Packages installation
8:36 Import libraries
11:52 Data ingestion
12:25 Chunking
12:57 Embedding model creation
13:30 Using Llama3 model via Groq
13:58 Creating and loading service context with Vector store index
16:01 Querying the Vector store with Query engine
17:49 Conclusion
18:32 Like Share and subscribe

LlamaIndex - https://docs.llamaindex.ai/en/stable/
Colab notebook - https://colab.research.google.com/drive/13WE752Z9Nf-1sQHohItLjlqT3JCqDgPY?usp=sharing
Groq - https://console.groq.com/
